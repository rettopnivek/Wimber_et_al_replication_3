---
title: "Replication analysis script"
author: "Kevin Potter"
date: "February 8, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document provides the analysis script for the pre-registered study "A replication of the behavioral paradigm for Wimber et al. (2015)." The pre-registration is available at https://AsPredicted.org/u5z76.pdf.

## Load in useful packages and functions

```{r,message=FALSE,warning=FALSE}
# Clear workspace
rm(list = ls())

# Define some useful functions
source('F0_Useful_functions.R')
```

## Load in data

```{r}
# Replication data
 load( 'Data/Wimber_rep_3.RData' )
```

## Initial training accuracy

```{r}
# Extract data for initial training sesssions
sel = allData$Cond == 2 | allData$Cond == 3 | allData$Cond == 4
curData = allData[ sel, ]

# Extract proportion correct by subject and condition
tmp = aggregate( curData$Accuracy, list(
  curData$Cond, curData$Subject ), mean )

# Determine the descriptive statistics across conditions
InitialTraining = aggregate( tmp$x, list( tmp[,1] ), S )
InitialTraining = cbind( c(1,1,2), c(1,2,1), InitialTraining$x )
colnames( InitialTraining ) = c('Associate','Repetition','Mean',
                                'SEM','IQR','Min','Max')
print( round( InitialTraining, 2 ) )
```

## Selective retrieval

```{r}
# Extract data for selective retrieval stage
SR = Extract_SR( allData[ allData$Cond == 5, ] )
```

### Estimate uncertainty around category performance

Here are the descriptive statistics for group-level performance:

```{r}
# Descriptive statistics for group-level performance
propCat = aggregate( SR$Choice, list( SR$Subject ), 
                     resp_cat  )
colnames( propCat$x ) = c('Target','Competitor','Error','Unknown')
colnames( propCat ) = c('S','Category')

DescStat = aggregate( propCat$Category, 
                      list( rep(1,nrow(propCat) ) ), S )
DescStat = matrix( unlist( DescStat[1,-1] ), 4, 5, byrow = T )
colnames( DescStat ) = c('Mean','SEM','IQR','Min','Max')
rownames( DescStat ) = c('Target','Competitor','Error','Unknown')
print( round( DescStat, 2 ) )
```

We need an estimate of the uncertainty around the proportions in which subjects correctly picked the target images, incorrectly picked the competitor images (i.e. intrusions), picked the third category (i.e. errors), or indicated that they did not know (i.e. unknown). To quantify this uncertainty, we fit a categorical logit model to the data:

```{r}
# Define priors based on previous results of Wimber et al. (2015)
Priors = cbind( c( 1.700, -.409, -1.700 ),
                c( .7, .7, .7 ),
                c( 2, 2, 2 ),
                c( 8, 8, 8 ) )

# Define a list of input for Stan
stan_dat = list(
  No = length( SR$Choice ),
  Ns = N,
  K = 4,
  Y = SR$Choice,
  indS = SR$Subject,
  Priors = Priors
)

warm = 750 # Iterations of warm-up
# Number of samples to approximate posterior per chain
niter = 1250*3
chains = 8 # Number of chains to run in parallel

startTime = Sys.time() # To assess run-time

# Compile the model
sm = stan_model(stanc_ret = stanc_builder(
  "Stan_scripts/SR_categories.stan"))

# Draw samples
fit = sampling( sm, data = stan_dat, 
                warmup = warm,
                iter = warm + niter, 
                chains = chains,
                thin = 3, 
                seed = 5001, # For reproducibility
                control = list(
                  adapt_delta = .95 )
)

post = extract(fit)

# Report run time
runTime = Sys.time() - startTime
print( runTime )

startTime = Sys.time() # To assess run-time

# Carry out posterior retrodictive checks
# Define a list of input for Stan
stan_dat_prc = list(
  No = length( SR$Choice ),
  Ns = N,
  K = 4,
  Ni = nrow( post$mu_beta ),
  indS = SR$Subject,
  mu_beta = post$mu_beta,
  sigma_beta = post$sigma_beta,
  beta_raw = post$beta_raw
)

# Compile the model
sm = stan_model(stanc_ret = stanc_builder(
  "Stan_scripts/SR_categories_sim.stan"))

# Draw samples
check = sampling( sm, data = stan_dat_prc, 
                  warmup = 0,
                  iter = 1, 
                  chains = 1,
                  seed = 1847, # For reproducibility
                  algorithm = 'Fixed_param'
)

# Extract simulated means
prc = extract( check )

# Report run time
runTime = Sys.time() - startTime
print( runTime )
```

```{r,echo=FALSE}

# Plot of predicted proportions versus observed
plot( c(.5,4.5), c(0,1), type = 'n', xlab = 'Condition',
      xaxt = 'n', ylab = 'Proportion', bty = 'n',
      main = 'Selective retrieval proportions' )
axis( 1, 1:4, c('Targets','Intrusions','Errors','Unknown'),
      tick = F )

for ( i in 1:4 ) {
  violinPlot( prc$P[1,,i], i, scaleH = .4,
              col = 'grey' )
}

# Add in observed means
segments( 1:4 - .4, DescStat[,1],
          1:4 + .4, DescStat[,1], lwd = 2, col = 'blue' )

# Proportions from original study
orig_prop = c(.747, .091, .025, .137)
segments( 1:4 - .4, orig_prop,
          1:4 + .4, orig_prop, lwd = 2, col = 'red' )

legend( 'topright', c('Observed','Predicted','Original'),
        fill = c('blue','grey','red'), bty = 'n' )
```

We can examine how close performance in the selective retrieval phase matched that of the original study:

```{r}
comp_vals = orig_prop

# Compute proportion of predicted proportions that 
# fall below observed proportions from Wimber et al. 
print( paste( 'P(Target) from Wimber et al. > Predicted: ',
              round( sum( comp_vals[1] > 
                            prc$P[1,,1] )/nrow( prc$P[1,,] ), 3 ),
              sep = '' ) )
print( paste( 'P(Competitor) from Wimber et al. > predicted: ',
              round( sum( comp_vals[2] > 
                            prc$P[1,,2] )/nrow( prc$P[1,,] ), 3 ),
              sep = '' ) )
print( paste( 'P(Error) from Wimber et al. > predicted: ',
              round( sum( comp_vals[2] > 
                            prc$P[1,,3] )/nrow( prc$P[1,,] ), 3 ),
              sep = '' ) )
```

### Trend analysis

To examine trends over cue-repetitions in the selective retrieval task, we can collapse the choices into binary data (e.g. 'targets' versus 'other', or 'intrusions' versus 'other') and use logistic regression:

```{r}
cD = allData[ allData$Cond == 5, ]

# Determine the frequencies for a given response type
Total = aggregate( rep(1,nrow(cD)), list(
  cD$CueRep - 2.5, cD$Subject ), sum )
Targets_rep = aggregate( SR$Choice == 1, list(
  cD$CueRep - 2.5, cD$Subject ), sum )
Competitors_rep = aggregate( SR$Choice == 2, list(
  cD$CueRep - 2.5, cD$Subject ), sum )
```

#### Targets

```{r}
# Create data frame
d = Targets_rep
colnames( d ) = c('R','S','Y')
d$S = as.factor( d$S )
d$N = Total$x

# Fit the model, collapsing over responses to give 
# targets vs. all others
fit = stan_glmer(cbind(Y, N - Y) ~ R + (1|S), 
                 data = d, family = binomial("logit"), 
                 prior_intercept = normal(1.39,.3), 
                 prior = normal( .3, .3 ), 
                 chains = 8, cores = 8, seed = 5019,
                 iter=1250,
                 warmup=500)

# Extract the posterior estimates
post = as.matrix( fit )
```

```{r,echo=FALSE}
# Plot marginal posterior distribution for slope
layout( cbind( 1, 2 ) )
tmp = hist( post[,2], breaks = 40, col = 'grey', 
            border = 'white', freq = F,
            xlab = 'Slope for linear trend', 
            main = 'Targets versus others' )

# Simulate data from the posterior estimates
nd = d; nd$Y = 0;
Sim = posterior_predict( fit, newdata = nd )
prc = apply( Sim, 1, function(x) aggregate(x/54, list(d$R), mean )$x )
ui = apply( prc, 1, quantile, prob = c( .025, .5, .975 ) )

# Plot predicted trend against observed data
obs = aggregate( d$Y/d$N, list( d$R ), mean )

plot( c(1,4), c(0,1), type = 'n', ylab = 'P(Target)',
      xlab = 'Repetitions', xaxt = 'n', bty = 'l',
      main = 'Correct recall over repetitions' )
axis( 1, 1:4, c('1st','2nd','3rd','4th'), tick = F )

polygon( c( 1:4, 4:1 ), c( ui[1,1:4], ui[3,4:1] ),
         col = 'grey', border = NA )
lines( 1:4, ui[2,], col = 'blue' )
points( 1:4, obs$x, pch = 19 )
```

#### Intrusions

```{r}
# Create data frame
d = Competitors_rep
colnames( d ) = c('R','S','Y')
d$S = as.factor( d$S )
d$N = Total$x

# Fit the model, collapsing over responses to give 
# targets vs. all others
fit = stan_glmer(cbind(Y, N - Y) ~ R + (1|S), 
                 data = d, family = binomial("logit"), 
                 prior_intercept = normal(-1.76,.3), 
                 prior = normal( -.3, .3 ), 
                 chains = 8, cores = 8, seed = 5019,
                 iter=1250,
                 warmup=500)

# Extract the posterior estimates
post = as.matrix( fit )
```

```{r,echo=FALSE}
# Plot marginal posterior distribution for slope
layout( cbind( 1, 2 ) )
tmp = hist( post[,2], breaks = 40, col = 'grey', 
            border = 'white', freq = F,
            xlab = 'Slope for linear trend', 
            main = 'Intrusions versus others' )

# Simulate data from the posterior estimates
nd = d; nd$Y = 0;
Sim = posterior_predict( fit, newdata = nd )
prc = apply( Sim, 1, function(x) aggregate(x/54, list(d$R), mean )$x )
ui = apply( prc, 1, quantile, prob = c( .025, .5, .975 ) )

# Plot predicted trend against observed data
obs = aggregate( d$Y/d$N, list( d$R ), mean )

plot( c(1,4), c(0,1), type = 'n', ylab = 'P(Intrusion)',
      xlab = 'Repetitions', xaxt = 'n', bty = 'l',
      main = 'Intrusions over repetitions' )
axis( 1, 1:4, c('1st','2nd','3rd','4th'), tick = F )

polygon( c( 1:4, 4:1 ), c( ui[1,1:4], ui[3,4:1] ),
         col = 'grey', border = NA )
lines( 1:4, ui[2,], col = 'blue' )
points( 1:4, obs$x, pch = 19 )
```

## Final recognition test

To examine whether we successfully replicated the results of the previous study, we fit a generalized linear model with logit link function to the accuracy performance of subjects in the final recognition memory test.

Note that in the pre-registration script, the priors were planned to be N(1.76,0.11) for the intercept, and N(-.29,.09) for the RIF effect. The priors are the marginal posterior distributions from the model applied to the original data. However, the priors given in the original script were based on a model that included an additional coefficient for a practice effect. When applying a model with only the RIF effect, the marginal posteriors (and therefore the appropriate priors) are in fact N(1.57,0.12) and N(-.28,0.08), respectively.

```{r}
# Extract data for final recognition test
sel = allData$Cond == 6
cD = allData[ sel, ]

# Create a data frame for data to be fitted
d = cD
# Dependent variable
d$Y = d$Accuracy
# Set missing data to be incorrect
d$Y[ is.na( d$RT ) ] = 0
# Image type (1 = target, 2 = competitor)
d$IT = as.factor( d$ImageType )
# Selective retrieval ( 1 = yes, 0 = no )
d$SR = as.factor( 1 - d$Baseline )
d$S = as.factor( d$Subject )
d$I = createIncrement( d$ImageNum )
# Dummy coded variable for RIF
d$RIF = 0; d$RIF[ d$IT == 2 & d$SR == 1 ] = 1

frm = aggregate(
  d$Y, list( d$IT, d$SR ), mean )

# Fit the model to the replication data
fit = stan_glmer( Y ~ RIF + (1|S) + (1|I), 
                  data = d, family = binomial("logit"), 
                  prior_intercept = normal(1.57,.12), 
                  prior = normal( -.28, .08 ), 
                  chains = 8, cores = 8, seed = 3874,
                  iter = 1250, warmup = 500 )

# Extract the posterior estimates
post = as.matrix( fit )
```


Before we evaluate the posterior estimates, it is important to conduct a posterior retrodictive check and ensure our model fit the observed data, by simulating data from the posterior estimates:

```{r}
# Generate posterior simulations
nd = d[,c('Y','RIF','S','I','IT','SR')]; nd$Y = 0;
sim = posterior_predict( fit )

# Calculate avg. accuracy over conditions of interest
f = function(x) {
  out = numeric(4)
  out[1] = mean( x[ nd$IT == 1 & nd$SR == 0 ] )
  out[2] = mean( x[ nd$IT == 2 & nd$SR == 0 ] )
  out[3] = mean( x[ nd$IT == 1 & nd$SR == 1 ] )
  out[4] = mean( x[ nd$IT == 2 & nd$SR == 1 ] )
  
  return( out )
}
prc = apply( sim, 1, f )

# Calculate the observed accuracy
obs = aggregate( d$Y, list( d$IT, d$SR ), mean )
colnames( obs ) = c( 'IT', 'SR', 'P' )
```

We can then see if the observed test summaries fall within the credible intervals for the model retrodictions, providing a test of the model's goodness of fit. If the model fails to fit the data, then regardless of the coefficient values, there has been a failure to replicate.

```{r,echo=FALSE}
plot( c(.4,2.4), c( .6, .9 ), type = 'n', bty = 'l', 
      xlab = 'Condition', ylab = 'Accuracy', xaxt = 'n',
      main = "Model's goodness of fit" )
axis( 1, 1:2, c( 'Baseline', 'Selective-retrieval' ),
      tick = F )

# Add in posterior retrodictive checks
ui = apply( prc, 1, quantile, prob = c( .025, .5, .975 ) )
xa = c( 1.1, .9, 1.9, 2.1 )
for ( i in 1:4 ) {
  polygon( xa[i] + c( -.05, -.05, .05, .05 ),
           ui[ c( 1, 3, 3, 1 ), i ], col = 'grey', 
           border = NA )
}

# Plot observed data
segments( c( .9, 1.1 ), obs$P[ c(2,1) ],
          c( 2.1, 1.9 ), obs$P[ c(4,3) ], lty = 1:2,
          lwd = 2 )
points( c(.9,2.1), obs$P[ c(2,4) ], pch = 21, bg = 'white', 
        cex = 1.5 )
points( c(1.1,1.9), obs$P[ c(1,3) ], pch = 22, bg = 'black', 
        cex = 1.5 )

# Add in original data
orig_dat = c( .797, .821, .786, .752 )
points( xa, orig_dat, pch = 19,
        col = 'red', cex = 1.5 )

legend( 'bottomright', c( 'Targets', 'Competitors', 'Original' ),
        pch = c(21,22,19), col = c('black','black','red'), 
        pt.bg = c('black','white','black'), bty = 'n' )
```

As can be seen, the model is unable to account for the observed trends in the data, indicating that we failed to replicate. There is therefore no point in checking the posterior estimates.