---
title: "Replication analysis script"
author: "Kevin Potter"
date: "February 8, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document provides the analysis script for the pre-registered study "A replication of the behavioral paradigm for Wimber et al. (2015)." The pre-registration is available at https://AsPredicted.org/u5z76.pdf.

## Load in useful packages and functions

```{r,message=FALSE,warning=FALSE}
# Clear workspace
rm(list = ls())

# For geting github packages
# install.packages(devtools)
# library(devtools)

# Miscellanous functions for modeling/plotting
# install_github("rettopnivek/utilityf")
library(utilityf)

# Load in packages for Bayesian estimation
# install.packages('rstan')
library(rstan)
# For parallel processing
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
# install.packages( 'rstanarm' )
library( rstanarm )

# Define some useful functions
source('F1_Useful_functions.R')
```

## Load in data

```{r}
# Simulated data for development purposes
# load( 'Data/All_subject_data_Sim.RData' )

# Replication data
 load( 'Data/Wimber_rep_3.RData' )
```

## Initial training accuracy

```{r}
# Extract data for initial training sesssions
sel = allData$Cond == 2 | allData$Cond == 3 | allData$Cond == 4
curData = allData[ sel, ]

# Extract proportion correct by subject and condition
tmp = aggregate( curData$Accuracy, list(
  curData$Cond, curData$Subject ), mean )

# Determine the descriptive statistics across conditions
InitialTraining = aggregate( tmp$x, list( tmp[,1] ), S )
InitialTraining = cbind( c(1,1,2), c(1,2,1), InitialTraining$x )
colnames( InitialTraining ) = c('Associate','Repetition','Mean',
                                'SEM','IQR','Min','Max')
print( round( InitialTraining, 2 ) )
```

## Selective retrieval

```{r}
# Extract data for selective retrieval stage
sel = allData$Cond == 5
curData = allData[sel,]

# Category for target images
Targets = floor(curData$Category)
# Category for competitor images
Competitors = round( 10*(curData$Category - Targets) )
# The category of the unrelated error
Errors = apply( cbind( Targets, Competitors ), 1, 
                function(x) { tar = 1:3 %in% x[1]; com = 1:3 %in% x[2];
                err = 1 - (tar+com); (1:3)[err==1] } )
# If unknown, subjects could pick a 4th response category
Unknown = rep( 4, nrow( curData ) )

# Determine a subject's choice based on categories
Choice = rep( NA, nrow( curData ) )
Choice[ curData$Resp == Targets ] = 1
Choice[ curData$Resp == Competitors ] = 2
Choice[ curData$Resp == Errors ] = 3
Choice[ curData$Resp == Unknown ] = 4
# Set missing data to be 'Unknown' as well'
Choice[ curData$Resp == 0 ] = 4
```

### Estimate uncertainty around category performance

Here are the descriptive statistics for group-level performance:

```{r}
# Define function to categorize responses
resp_cat = function(x) {
  out = c(
    sum( x == 1 ),
    sum( x == 2 ),
    sum( x == 3 ),
    sum( x == 4 ) )
  out = out/length(x)
  
  return( out )
}
# Descriptive statistics for group-level performance
propCat = aggregate( Choice, list( curData$Subject ), 
                     resp_cat  )
colnames( propCat$x ) = c('Target','Competitor','Error','Unknown')
colnames( propCat ) = c('S','Category')

DescStat = aggregate( propCat$Category, 
                      list( rep(1,nrow(propCat) ) ), S )
DescStat = matrix( unlist( DescStat[1,-1] ), 4, 5, byrow = T )
colnames( DescStat ) = c('Mean','SEM','IQR','Min','Max')
rownames( DescStat ) = c('Target','Competitor','Error','Unknown')
print( round( DescStat, 2 ) )
```

We need an estimate of the uncertainty around the proportions in which subjects correctly picked the target images, incorrectly picked the competitor images (i.e. intrusions), picked the third category (i.e. errors), or indicated that they did not know (i.e. unknown). To quantify this uncertainty, we fit a categorical logit model to the data:

```{r}
# Define priors based on previous results of Wimber et al. (2015)
Priors = cbind( c( 1.700, -.409, -1.700 ),
                c( .3, .3, .3 ),
                c( 2, 2, 2 ),
                c( 8, 8, 8 ) )

# Define a list of input for Stan
stan_dat = list(
  No = length( Choice ),
  Ns = N,
  K = 4,
  Y = Choice,
  indS = curData$Subject,
  Priors = Priors
)

warm = 750 # Iterations of warm-up
niter = 1250*3 # Number of samples to approximate posterior per chain
chains = 8 # Number of chains to run in parallel

startTime = Sys.time() # To assess run-time

# Compile the model
sm = stan_model(stanc_ret = stanc_builder(
  "Stan_scripts/SR_categories.stan"))

# Draw samples
fit = sampling( sm, data = stan_dat, 
                warmup = warm,
                iter = warm + niter, 
                chains = chains,
                thin = 3, 
                seed = 1739, # For reproducibility
                control = list(
                  adapt_delta = .95 )
)

post = extract(fit)
# Report run time
runTime = Sys.time() - startTime
print( runTime )
```

```{r,echo=FALSE}
# Plot of predicted proportions versus observed
plot( c(.5,4.5), c(0,1), type = 'n', xlab = 'Condition',
      xaxt = 'n', ylab = 'Proportion', bty = 'n',
      main = 'Selective retrieval proportions' )
pred = apply( post$theta, 1, colMeans )
axis( 1, 1:4, c('Targets','Intrusions','Errors','Unknown'),
      tick = F )

for ( i in 1:4 ) {
  violinPlot( pred[i,], i, scaleH = .4,
              col = 'grey' )
}

# Add in observed means
segments( 1:4 - .4, DescStat[,1],
          1:4 + .4, DescStat[,1], lwd = 2, col = 'blue' )

# Proportions from original study
orig_prop = c(.747, .091, .025, .137)
segments( 1:4 - .4, orig_prop,
          1:4 + .4, orig_prop, lwd = 2, col = 'red' )

legend( 'topright', c('Observed','Predicted','Original'),
        fill = c('blue','grey','red'), bty = 'n' )
```

We can examine how close performance in the selective retrieval phase matched that of the original study:

```{r}
comp_vals = reverseSoftmax( orig_prop, 
                            restrict = c( F, F, F, T ))$par[1:3]
# Extract credible intervals
ci = HDInterval::hdi( post$mu_beta )
print( paste( 'P(Target) from Wimber et al. > Posterior: ',
              round( sum( comp_vals[1] > 
                            post$mu_beta[,1] )/nrow( post$mu_beta ), 3 ),
              sep = '' ) )
print( paste( 'P(Competitor) from Wimber et al. > Posterior: ',
              round( sum( comp_vals[2] > 
                            post$mu_beta[,2] )/nrow( post$mu_beta ), 3 ),
              sep = '' ) )
print( paste( 'P(Error) from Wimber et al. > Posterior: ',
              round( sum( comp_vals[3] > 
                            post$mu_beta[,3] )/nrow( post$mu_beta ), 3 ),
              sep = '' ) )
```

### Trend analysis

To examine trends over cue-repetitions in the selective retrieval task, we can collapse the choices into binary data (e.g. 'targets' versus 'other', or 'intrusions' versus 'other') and use logistic regression:

```{r}
# Determine the frequencies for a given response type
Total = aggregate( rep(1,nrow(curData)), list(
  curData$CueRep - 2.5, curData$Subject ), sum )
Targets_rep = aggregate( Choice == 1, list(
  curData$CueRep - 2.5, curData$Subject ), sum )
Competitors_rep = aggregate( Choice == 2, list(
  curData$CueRep - 2.5, curData$Subject ), sum )
```

#### Targets

```{r}
# Create data frame
d = Targets_rep
colnames( d ) = c('R','S','Y')
d$S = as.factor( d$S )
d$N = Total$x

# Fit the model, collapsing over responses to give 
# targets vs. all others
fit = stan_glmer(cbind(Y, N - Y) ~ R + (1|S), 
                 data = d, family = binomial("logit"), 
                 prior_intercept = normal(1.39,.3), 
                 prior = normal( .3, .3 ), 
                 chains = 8, cores = 8, seed = 5019,
                 iter=1250,
                 warmup=500)

# Extract the posterior estimates
post = as.matrix( fit )
```

```{r,echo=FALSE}
# Plot marginal posterior distribution for slope
layout( cbind( 1, 2 ) )
tmp = hist( post[,2], breaks = 40, col = 'grey', 
            border = 'white', freq = F,
            xlab = 'Slope for linear trend', 
            main = 'Targets versus others' )

# Simulate data from the posterior estimates
nd = d; nd$Y = 0;
Sim = posterior_predict( fit, newdata = nd )
prc = apply( Sim, 1, function(x) aggregate(x/54, list(d$R), mean )$x )
ui = apply( prc, 1, quantile, prob = c( .025, .5, .975 ) )

# Plot predicted trend against observed data
obs = aggregate( d$Y/d$N, list( d$R ), mean )

plot( c(1,4), c(0,1), type = 'n', ylab = 'P(Target)',
      xlab = 'Repetitions', xaxt = 'n', bty = 'l',
      main = 'Correct recall over repetitions' )
axis( 1, 1:4, c('1st','2nd','3rd','4th'), tick = F )

polygon( c( 1:4, 4:1 ), c( ui[1,1:4], ui[3,4:1] ),
         col = 'grey', border = NA )
lines( 1:4, ui[2,], col = 'blue' )
points( 1:4, obs$x, pch = 19 )
```

#### Intrusions

```{r}
# Create data frame
d = Competitors_rep
colnames( d ) = c('R','S','Y')
d$S = as.factor( d$S )
d$N = Total$x

# Fit the model, collapsing over responses to give 
# targets vs. all others
fit = stan_glmer(cbind(Y, N - Y) ~ R + (1|S), 
                 data = d, family = binomial("logit"), 
                 prior_intercept = normal(-1.76,.3), 
                 prior = normal( -.3, .3 ), 
                 chains = 8, cores = 8, seed = 5019,
                 iter=1250,
                 warmup=500)

# Extract the posterior estimates
post = as.matrix( fit )
```

```{r,echo=FALSE}
# Plot marginal posterior distribution for slope
layout( cbind( 1, 2 ) )
tmp = hist( post[,2], breaks = 40, col = 'grey', 
            border = 'white', freq = F,
            xlab = 'Slope for linear trend', 
            main = 'Intrusions versus others' )

# Simulate data from the posterior estimates
nd = d; nd$Y = 0;
Sim = posterior_predict( fit, newdata = nd )
prc = apply( Sim, 1, function(x) aggregate(x/54, list(d$R), mean )$x )
ui = apply( prc, 1, quantile, prob = c( .025, .5, .975 ) )

# Plot predicted trend against observed data
obs = aggregate( d$Y/d$N, list( d$R ), mean )

plot( c(1,4), c(0,1), type = 'n', ylab = 'P(Intrusion)',
      xlab = 'Repetitions', xaxt = 'n', bty = 'l',
      main = 'Intrusions over repetitions' )
axis( 1, 1:4, c('1st','2nd','3rd','4th'), tick = F )

polygon( c( 1:4, 4:1 ), c( ui[1,1:4], ui[3,4:1] ),
         col = 'grey', border = NA )
lines( 1:4, ui[2,], col = 'blue' )
points( 1:4, obs$x, pch = 19 )
```

## Final recognition test

To examine whether we successfully replicated the results of the previous study, we fit a generalized linear model with logit link function to the accuracy performance of subjects in the final recognition memory test.

Note that in the pre-registration script, the priors were planned to be N(1.76,0.11) for the intercept, and N(-.29,.09) for the RIF effect. The priors are the marginal posterior distributions from the model applied to the original data. However, the priors given in the original script were based on a model that included an additional coefficient for a practice effect. When applying a model with only the RIF effect, the marginal posteriors (and therefore the appropriate priors) are in fact N(1.57,0.12) and N(-.28,0.08), respectively.

```{r}
# Extract data for final recognition test
sel = allData$Cond == 6
curData = allData[ sel, ]

# Create a data frame for data to be fitted
d = curData
# Dependent variable
d$Y = d$Accuracy
# Image type (1 = target, 2 = competitor)
d$IT = as.factor( d$ImageType )
# Selective retrieval ( 1 = yes, 0 = no )
d$SR = as.factor( 1 - d$Baseline )
d$S = as.factor( d$Subject )
d$I = as.factor( d$ImageNum )
# Nuisance parameter for bernoulli distribution
d$Trial = rep(1,nrow(d))
# Dummy coded variable for RIF
d$RIF = 0; d$RIF[ d$IT == 2 & d$SR == 1 ] = 1

# Fit the model to the original data
fit = stan_glmer(cbind(Y, Trial) ~ RIF + (1|S) + (1|I), 
                 data = d, family = binomial("logit"), 
                 prior_intercept = normal(1.57,.12), 
                 prior = normal( -.28, .08 ), 
                 chains = 8, cores = 8, seed = 3874,
                 iter=1250,warmup=500)

# Extract the posterior estimates
post = as.matrix( fit )
```

We now check to see if posterior mean falls within the credible interval from the previous study:

```{r}
print( paste( 'Successful replication:', mean( post[,2] ) < -.11 ) )
```

```{r,echo=FALSE}
plot( c(0,1), c(-1,1), type = 'n', xlab = 'Estimate for RIF effect',
      main = ' ', bty = 'l', ylab = 'Posterior values' )
violinPlot( post[,2], pos = .5, scaleH = .4, crit = -.11,
            border = NA, col = 'grey' )
violinPlot( post[,2], pos = .5, scaleH = .4 )
legend( 'topright', paste( 'P(Posterior > -.11) =', 
                           round( mean( post[,2] > -.11 ), 3 ) ),
        bty = 'n' )
```

We will also conduct a posterior retrodictive check, by simulating data from the posterior estimates:

```{r}
nd = d[,c('Y','Trial','RIF','S','I','IT','SR')]; nd$Y = 0;
sim = posterior_predict( fit, newdata = nd )

# Calculate avg. accuracy over conditions of interest
prc = apply( sim, 1, function(x) 
  aggregate( x, list( nd$IT, nd$SR ), mean )$x )
# Calculate the observed accuracy
obs = aggregate( d$Y, list( d$IT, d$SR ), mean )
colnames( obs ) = c( 'IT', 'SR', 'P' )
```

We can then see if the observed test summaries fall within the credible intervals for the model retrodictions, providing a test of the model's goodness of fit. If the model fails to fit the data, then regardless of the coefficient values, there has been a failure to replicate.

```{r,echo=FALSE}
plot( c(.4,2.4), c( .6, .9 ), type = 'n', bty = 'l', 
      xlab = 'Condition', ylab = 'Accuracy', xaxt = 'n',
      main = "Model's goodness of fit" )
axis( 1, 1:2, c( 'Baseline', 'Selective-retrieval' ),
      tick = F )

# Add in posterior retrodictive checks
ui = apply( prc, 1, quantile, prob = c( .025, .5, .975 ) )
xa = c( 1.1, .9, 1.9, 2.1 )
for ( i in 1:4 ) {
  polygon( xa[i] + c( -.05, -.05, .05, .05 ),
           ui[ c( 1, 3, 3, 1 ), i ], col = 'grey', 
           border = NA )
}

# Plot observed data
segments( c( .9, 1.1 ), obs$P[ c(2,1) ],
          c( 2.1, 1.9 ), obs$P[ c(4,3) ], lty = 1:2,
          lwd = 2 )
points( c(.9,2.1), obs$P[ c(2,4) ], pch = 21, bg = 'white', 
        cex = 1.5 )
points( c(1.1,1.9), obs$P[ c(1,3) ], pch = 22, bg = 'black', 
        cex = 1.5 )

# Add in original data
orig_dat = c( .797, .821, .786, .752 )
points( xa, orig_dat, pch = 19,
        col = 'red', cex = 1.5 )

legend( 'bottomright', c( 'Targets', 'Competitors', 'Original' ),
        pch = c(21,22,19), col = c('black','black','red'), 
        pt.bg = c('black','white','black'), bty = 'n' )
```
